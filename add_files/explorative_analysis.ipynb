{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install stanza\n",
    "\n",
    "import pandas as pd\n",
    "import stanza, pickle\n",
    "import statistics\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "! pip install textstat\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_train = pd.read_csv(\"eng/train/train.csv\", header = 0)\n",
    "tur_train = pd.read_csv(\"tur/train/train.csv\", header = 0)\n",
    "\n",
    "eng_test = pd.read_csv(\"eng/test/test.csv\", header = 0)\n",
    "tur_test = pd.read_csv(\"tur/test/test.csv\", header = 0)\n",
    "\n",
    "print(eng_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column \"Time\" should have been \"Author\"! Also let's change the name for \"Publication Date\" to \"Time\" as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column name error fix\n",
    "\n",
    "eng_train1 = eng_train.rename(columns={\"Time\": \"Author\", \"Publication Date\": \"Time\"})\n",
    "tur_train1 = tur_train.rename(columns={\"Time\": \"Author\", \"Publication Date\": \"Time\"})\n",
    "\n",
    "eng_test1 = eng_test.rename(columns={\"Time\": \"Author\", \"Publication Date\": \"Time\"})\n",
    "tur_test1 = tur_test.rename(columns={\"Time\": \"Author\", \"Publication Date\": \"Time\"})\n",
    "\n",
    "eng_train1.head() # fixed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_train1.head() # fixed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of English article dataset:\", eng_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Turkish article dataset:\", tur_train1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_train1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_train1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Own Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_str_data(df, col_name):\n",
    "\n",
    "    lengths = []\n",
    "    for str_data in df[col_name]:\n",
    "        lengths.append(len(str_data))\n",
    "    mean = sum(lengths)/len(lengths)\n",
    "    return str(mean)\n",
    "\n",
    "\n",
    "def max_str_data(df, col_name):\n",
    "\n",
    "    lengths = []\n",
    "    for str_data in df[col_name]:\n",
    "        lengths.append(len(str_data))\n",
    "    mean = max(lengths)\n",
    "    return str(mean)\n",
    "\n",
    "\n",
    "def min_str_data(df, col_name):\n",
    "\n",
    "    lengths = []\n",
    "    for str_data in df[col_name]:\n",
    "        lengths.append(len(str_data))\n",
    "    mean = min(lengths)\n",
    "    return str(mean)\n",
    "\n",
    "\n",
    "print(\"Mean length of English titles: \" + mean_str_data(eng_train1, \"Title\"))\n",
    "print(\"Mean length of Turkish titles: \" + mean_str_data(tur_train1, \"Title\"))\n",
    "print(\"------------------------------------\")\n",
    "print(\"Length of longest English title: \" + max_str_data(eng_train1, \"Title\"))\n",
    "print(\"Length of longest Turkish title: \" + max_str_data(tur_train1, \"Title\"))\n",
    "print(\"------------------------------------\")\n",
    "print(\"Length of shortest English title: \" + min_str_data(eng_train1, \"Title\"))\n",
    "print(\"Length of shortest Turkish title: \" + min_str_data(tur_train1, \"Title\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding missing (NaN) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_train1.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All missing values are in column \"Author\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_train1.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost missing values are in column \"Author\". We also have a couple of missing articles in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_train_df = eng_train1.drop([\"Author\"], axis=1)\n",
    "tur_train_df1 = tur_train1.drop([\"Author\"], axis=1)\n",
    "\n",
    "eng_test_df = eng_test1.drop([\"Author\"], axis=1)\n",
    "tur_test_df1 = tur_test1.drop([\"Author\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both datasets, column \"Author\" is missing more than 50% of its values. It is best if we drop this variable all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_train_df.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_test_df.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows with empty articles in Turkish dataset will also be deleted - from my perspective, they would be useless for linguistic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_train_df = tur_train_df1.dropna()\n",
    "tur_test_df = tur_test_df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Statistics with Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the nlp pipeline\n",
    "stanza.download('en')\n",
    "stanza.download('tr')\n",
    "\n",
    "eng_nlp = stanza.Pipeline('en')\n",
    "tur_nlp = stanza.Pipeline('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract token frequencies info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_freqs_to_pickle(df, nlp_pipeline, frequency_file):\n",
    "    \n",
    "    # Initialize variables\n",
    "    articles = df[\"Text\"].values.tolist()\n",
    "\n",
    "    # Iterate through all articles\n",
    "    for article in articles:\n",
    "        \n",
    "        # Process the article with the stanza pipeline\n",
    "        processed_article = nlp_pipeline(article)\n",
    "\n",
    "        # Iterate through all sentences of the article\n",
    "        sentences = processed_article.sentences\n",
    "        token_frequencies = Counter()\n",
    "        \n",
    "        # Add the tokens to a counter\n",
    "        for sentence in sentences:\n",
    "            all_tokens =[token.text for token in sentence.tokens]\n",
    "            token_frequencies.update(all_tokens)\n",
    "\n",
    "    # Save the token frequencies as a pickle file\n",
    "    pickle.dump(token_frequencies, open(frequency_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_path = 'en_tokenfrequencies.pkl'\n",
    "tr_path = 'tr_tokenfrequencies.pkl'\n",
    "\n",
    "token_freqs_to_pickle(eng_train_df, eng_nlp, en_path)\n",
    "token_freqs_to_pickle(tur_train_df, tur_nlp, tr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get processed articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles_to_pickle(df, nlp_pipeline, frequency_file):\n",
    "    \n",
    "    # Initialize variables\n",
    "    articles = df[\"Text\"].values.tolist()\n",
    "\n",
    "    # Iterate through all articles\n",
    "    for article in articles:\n",
    "        \n",
    "        # Process the article with the stanza pipeline\n",
    "        processed_article = nlp_pipeline(article)\n",
    "\n",
    "    # Save the token frequencies as a pickle file\n",
    "    pickle.dump(processed_article, open(frequency_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_path2 = 'en_processedarticles.pkl'\n",
    "tr_path2 = 'tr_processedarticles.pkl'\n",
    "\n",
    "articles_to_pickle(eng_train_df, eng_nlp, en_path2)\n",
    "articles_to_pickle(tur_train_df, tur_nlp, tr_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linguistic Units / Quality of the Processed Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp_output = pickle.load(open(\"en_processedarticles.pkl\",\"rb\"))\n",
    "#print(en_nlp_output)\n",
    "tr_nlp_output = pickle.load(open(\"tr_processedarticles.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(en_nlp_output.sentences):\n",
    "    # Only check first 20 sentences\n",
    "    if i==20:\n",
    "        break\n",
    "        \n",
    "    print(str(i), sentence.text)\n",
    "    for word in sentence.words:\n",
    "        # To check lemmatization, base words are excluded\n",
    "        if not word.text == word.lemma:\n",
    "            print(word.id, word.text, word.lemma)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizations are almost correct for this particular sample (There is a particular error occuring when lemmatizing words ending with -ions e.g. \"characterizations\", \"perceptions\" and \"reparations\"). However, lemmatization of punctuation seems unnecessary and should be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(tr_nlp_output.sentences):\n",
    "    # Only check first 20 sentences\n",
    "    if i==20:\n",
    "        break\n",
    "        \n",
    "    print(str(i), sentence.text)\n",
    "    for word in sentence.words:\n",
    "        if not word.text == word.lemma:\n",
    "            print(word.id, word.text, word.lemma)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turkish lemmatization seems to be more erronous than the English lemmatization. Moreover, the error cases vary in terms of the cause of the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check POS-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_POS_tags(nlp_output):\n",
    "\n",
    "    token_pos_frequencies = Counter()\n",
    "\n",
    "    for sentence in nlp_output.sentences:\n",
    "        token_pos = [(word.lemma, word.pos) for word in sentence.words]\n",
    "        token_pos_frequencies.update(token_pos)\n",
    "        \n",
    "    print(token_pos_frequencies.most_common(50))\n",
    "\n",
    "\n",
    "check_POS_tags(en_nlp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_POS_tags(tr_nlp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both dataset, the most frequent words are punctuation! It is best to remove them as they offer limited linguistic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the effect of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \n",
    "                \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n",
    "                \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \n",
    "                \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \n",
    "                \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \n",
    "                \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \n",
    "                \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "tr_stopwords = [\"acaba\",\"ama\",\"aslında\",\"az\",\"bazı\",\"belki\",\"biri\",\"birkaç\",\"birşey\",\"biz\",\"bu\",\"çok\",\"çünkü\",\"da\",\"daha\",\"de\",\"defa\",\"diye\",\"eğer\",\"en\",\"gibi\",\"hem\",\n",
    "                \"hep\",\"hepsi\",\"her\",\"hiç\",\"için\",\"ile\",\"ise\",\"kez\",\"ki\",\"kim\",\"mı\",\"mu\",\"mü\",\"nasıl\",\"ne\",\"neden\",\"nerde\",\"nerede\",\"nereye\",\"niçin\",\"niye\",\"o\",\"sanki\",\n",
    "                \"şey\",\"siz\",\"şu\",\"tüm\",\"ve\",\"veya\",\"ya\",\"yani\"]\n",
    "\n",
    "\n",
    "def calculate_token_frequencies(nlp_output,stopwords,ignore_stopwords=False):\n",
    "    token_frequencies = Counter()\n",
    "    for sentence in nlp_output.sentences:\n",
    "        if ignore_stopwords:\n",
    "            tokens = [token.text for token in sentence.tokens if token.text not in stopwords ]\n",
    "        else:\n",
    "            tokens = [token.text for token in sentence.tokens]\n",
    "\n",
    "        token_frequencies.update(tokens)\n",
    "    return token_frequencies\n",
    "\n",
    "en_token_frequencies = calculate_token_frequencies(en_nlp_output,en_stopwords,ignore_stopwords=True)\n",
    "print(en_token_frequencies.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_token_frequencies = calculate_token_frequencies(tr_nlp_output,tr_stopwords,ignore_stopwords=True)\n",
    "print(tr_token_frequencies.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have more linguistic info regarding the data. But the effect of keeping the punctuation and uppercased words is evident!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_frequencies_dataset = pickle.load(open(\"en_tokenfrequencies.pkl\",\"rb\"))\n",
    "tr_frequencies_dataset = pickle.load(open(\"tr_tokenfrequencies.pkl\",\"rb\"))\n",
    "\n",
    "def print_normalized_freqs(frequencies_dataset, stopwords):\n",
    "\n",
    "    normalized_frequencies = Counter()\n",
    "    for token, freq in frequencies_dataset.items():\n",
    "        # Remove stopwords and punctuation\n",
    "        if not token in stopwords and not token in string.punctuation:\n",
    "            normalized_frequency = float(freq/frequencies_dataset[token])\n",
    "            normalized_frequencies[token] = normalized_frequency\n",
    "        \n",
    "    print(normalized_frequencies.most_common(100))\n",
    "\n",
    "print_normalized_freqs(en_frequencies_dataset, en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_normalized_freqs(tr_frequencies_dataset, tr_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that some stopwords might have been skipped due to being uppercased. Hence it might be beneficial to lowercase the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dep_pars(content):\n",
    "    sentences = content.sentences\n",
    "    for sentence in sentences[0:2]:\n",
    "        print(\"id\", \"token\", \"head id\", \"head token\", \"dependency relation\")\n",
    "        for word in sentence.words:\n",
    "            # word.head only provides the id of the word, here we determine the head token based on the id\n",
    "            if word.head == 0:\n",
    "                head_token =\"root\"\n",
    "            else:\n",
    "                head_token = sentence.words[word.head-1].text\n",
    "            # Output the dependency relation\n",
    "            print(word.id, word.text, word.head, head_token, word.deprel)\n",
    "\n",
    "print_dep_pars(en_nlp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dep_pars(tr_nlp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting stylistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_articles(df, language):\n",
    "\n",
    "    articles = df[\"Text\"]\n",
    "    nlp = stanza.Pipeline(language, processors='tokenize,pos,lemma')\n",
    "\n",
    "    # Process the articles\n",
    "    processed_articles =[]\n",
    "    for article in articles:\n",
    "        processed_articles.append(nlp.process(article))\n",
    "\n",
    "    return processed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_proc_arts = proc_articles(eng_train_df, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_proc_arts = proc_articles(tur_train_df, 'tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "\n",
    "def proc_articles_spacy(df):\n",
    "\n",
    "    articles = df[\"Text\"]\n",
    "    nlp =  en_core_web_sm.load()\n",
    "\n",
    "    # Process the articles\n",
    "    processed_articles =[]\n",
    "    for article in articles:\n",
    "        processed_articles.append(nlp(article))\n",
    "\n",
    "    return processed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_spacy_arts = proc_articles_spacy(eng_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def tr_proc_articles_spacy(df):\n",
    "\n",
    "    articles = df[\"Text\"]\n",
    "    nlp = spacy.load(\"tr_core_news_trf\")\n",
    "\n",
    "    # Process the articles\n",
    "    processed_articles =[]\n",
    "    for article in articles:\n",
    "        processed_articles.append(nlp(article))\n",
    "\n",
    "    return processed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_spacy_arts = proc_articles_spacy(tur_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lftk\n",
    "\n",
    "# en_features_list= ['t_word','t_stopword','t_uword','t_sent','a_syll_ps','a_word_ps','a_stopword_ps','fkre','fkgl','rt_fast','rt_average','rt_slow']\n",
    "\n",
    "ttr = []\n",
    "word_count=[]\n",
    "sent_count = []\n",
    "avg_sentence_len = []\n",
    "fkre = []\n",
    "\n",
    "for article in eng_proc_arts:\n",
    "\n",
    "    # Calculate TTR\n",
    "    token_frequencies = Counter()\n",
    "    for sentence in article.sentences:\n",
    "        all_tokens =[token.text for token in sentence.tokens]\n",
    "        token_frequencies.update(all_tokens)\n",
    "    num_types = len(token_frequencies.keys())\n",
    "    num_tokens = sum(token_frequencies.values())\n",
    "    tt_ratio = num_types/float(num_tokens)\n",
    "    ttr.append(tt_ratio)\n",
    "\n",
    "    # Calculate number of words in the text\n",
    "    words = 0\n",
    "    for sentence in article.sentences:\n",
    "        words += len([token for token in sentence.tokens])\n",
    "    word_count.append(words)\n",
    "\n",
    "    # Calculate number of sentences\n",
    "    sents = 0\n",
    "    sents += len([sentence for sentence in article.sentences])\n",
    "    sent_count.append(sents)\n",
    "\n",
    "    # Calculate average sentence length\n",
    "    sentence_lengths =[len(sentence.tokens) for sentence in article.sentences]\n",
    "    avg_sentence_len.append(statistics.mean(sentence_lengths))\n",
    "\n",
    "\n",
    "for article in en_spacy_arts:\n",
    "    LFTK = lftk.Extractor(docs = article)\n",
    "    LFTK.customize(stop_words=True, punctuations=False, round_decimal=3)\n",
    "    extracted_features = LFTK.extract(features = ['fkre'])\n",
    "    for key, value in extracted_features.items():\n",
    "        fkre.append(value)\n",
    "    \n",
    "\n",
    "# Add the information to the data frame\n",
    "eng_train_df[\"Type-Token Ratio\"] = ttr\n",
    "eng_train_df[\"Word Count\"] = word_count\n",
    "eng_train_df[\"Sentence Count\"] = sent_count\n",
    "eng_train_df[\"Avg Sentence Length\"] = avg_sentence_len\n",
    "eng_train_df[\"Flesch-Kincaid Reading Ease\"] = fkre\n",
    "\n",
    "eng_train_df.to_csv(\"en_stylistic_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lftk\n",
    "\n",
    "ttr = []\n",
    "avg_sentence_len = []\n",
    "avg_num_words = []\n",
    "word_count = []\n",
    "sent_count = []\n",
    "\n",
    "for article in tur_proc_arts:\n",
    "\n",
    "    # Calculate TTR\n",
    "    token_frequencies = Counter()\n",
    "    for sentence in article.sentences:\n",
    "        all_tokens =[token.text for token in sentence.tokens]\n",
    "        token_frequencies.update(all_tokens)\n",
    "    num_types = len(token_frequencies.keys())\n",
    "    num_tokens = sum(token_frequencies.values())\n",
    "    tt_ratio = num_types/float(num_tokens)\n",
    "    ttr.append(tt_ratio)\n",
    "\n",
    "    # Calculate number of words in the text\n",
    "    words = 0\n",
    "    for sentence in article.sentences:\n",
    "        words += len([token for token in sentence.tokens])\n",
    "    word_count.append(words)\n",
    "\n",
    "    # Calculate number of sentences\n",
    "    sents = 0\n",
    "    sents += len([sentence for sentence in article.sentences])\n",
    "    sent_count.append(sents)\n",
    "\n",
    "    # Calculate average sentence length\n",
    "    sentence_lengths =[len(sentence.tokens) for sentence in article.sentences]\n",
    "    avg_sentence_len.append(statistics.mean(sentence_lengths))\n",
    "\n",
    "\n",
    "for article in tr_spacy_arts:\n",
    "    LFTK = lftk.Extractor(docs = article)\n",
    "    LFTK.customize(stop_words=True, punctuations=False, round_decimal=3)\n",
    "    extracted_features = LFTK.extract(features = ['a_word_ps'])\n",
    "    for key, value in extracted_features.items():\n",
    "        avg_num_words.append(value)\n",
    "\n",
    "# Add the information to the data frame\n",
    "tur_train_df[\"Type-Token Ratio\"] = ttr\n",
    "tur_train_df[\"Word Count\"] = word_count\n",
    "tur_train_df[\"Sentence Count\"] = sent_count\n",
    "tur_train_df[\"Avg Sentence Length\"] = avg_sentence_len\n",
    "tur_train_df[\"Avg Num of Words Per Sent\"] = avg_num_words\n",
    "\n",
    "tur_train_df.to_csv(\"tr_stylistic_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability check is not working: will be fixed before final submission!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "time = [\"am\" if t.startswith(\"0\") else \"pm\" for t in eng_train_df[\"Time\"] ]\n",
    "\n",
    "# We transform the time stamps into a categorical value\n",
    "time = [\"am\" if t.startswith(\"0\") else \"pm\" for t in eng_train_df[\"Time\"] ]\n",
    "print(time)\n",
    "eng_train_df[\"Time Category\"] = time\n",
    "\n",
    "\n",
    "sb.lmplot(eng_train_df, x=\"Avg Sentence Length\", y=\"Type-Token Ratio\", hue=\"Country\", col=\"Time Category\", fit_reg = False )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More countries appeared to have outputted articles in English in the later hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma')\n",
    "tr_nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma')\n",
    "\n",
    "def eng_preprocess(article):\n",
    "    processed_article = en_nlp.process(article)\n",
    "    all_lemmas = []\n",
    "    for s in processed_article.sentences: \n",
    "        if len(s.text.strip())>0:\n",
    "            lemmas = [word.lemma.lower() for word in s.words if not word.lemma==None]\n",
    "            clean_lemmas = [lemma for lemma in lemmas if not lemma in en_stopwords and not lemma in string.punctuation]\n",
    "            all_lemmas.extend(clean_lemmas)\n",
    "    return all_lemmas\n",
    "\n",
    "def tur_preprocess(article):\n",
    "    processed_article = tr_nlp.process(article)\n",
    "    all_lemmas = []\n",
    "    for s in processed_article.sentences: \n",
    "        if len(s.text.strip())>0:\n",
    "            lemmas = [word.lemma.lower() for word in s.words if not word.lemma==None]\n",
    "            clean_lemmas = [lemma for lemma in lemmas if not lemma in tr_stopwords and not lemma in string.punctuation]\n",
    "            all_lemmas.extend(clean_lemmas)\n",
    "    return all_lemmas\n",
    "\n",
    "# Read data files\n",
    "eng_train_csv = \"eng/train/train.csv\"\n",
    "eng_train_df = pd.read_csv(eng_train_csv, keep_default_na=False, header=0)\n",
    "\n",
    "eng_test_csv = \"eng/test/test.csv\"\n",
    "eng_test_df = pd.read_csv(eng_test_csv, keep_default_na=False, header=0)\n",
    "\n",
    "tur_train_csv = \"tur/train/train.csv\"\n",
    "tur_train_df = pd.read_csv(tur_train_csv, keep_default_na=False, header=0)\n",
    "\n",
    "tur_test_csv = \"tur/test/test.csv\"\n",
    "tur_test_df = pd.read_csv(tur_test_csv, keep_default_na=False, header=0)\n",
    "\n",
    "# Filter out empty articles\n",
    "eng_train_filter = eng_train_df[eng_train_df[\"Text\"].str.len() >0 ]\n",
    "eng_train_articles = eng_train_filter[\"Text\"]\n",
    "\n",
    "eng_test_filter = eng_test_df[eng_test_df[\"Text\"].str.len() >0 ]\n",
    "eng_test_articles = eng_test_filter[\"Text\"]\n",
    "\n",
    "tur_train_filter = tur_train_df[tur_train_df[\"Text\"].str.len() >0 ]\n",
    "tur_train_articles = tur_train_filter[\"Text\"]\n",
    "\n",
    "tur_test_filter = tur_test_df[tur_test_df[\"Text\"].str.len() >0 ]\n",
    "tur_test_articles = tur_test_filter[\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# You can play around with the ngram range\n",
    "eng_vectorizer = TfidfVectorizer(use_idf=True, tokenizer=eng_preprocess)\n",
    "eng_train_tf_idf = eng_vectorizer.fit_transform(eng_train_articles)\n",
    "eng_train_all_terms = eng_vectorizer.get_feature_names_out()\n",
    "\n",
    "eng_test_tf_idf = eng_vectorizer.fit_transform(eng_test_articles)\n",
    "eng_test_all_terms = eng_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_vectorizer = TfidfVectorizer(use_idf=True, tokenizer=tur_preprocess)\n",
    "tur_train_tf_idf = tur_vectorizer.fit_transform(tur_train_articles)\n",
    "tur_train_all_terms = eng_vectorizer.get_feature_names_out()\n",
    "\n",
    "eng_test_tf_idf = tur_vectorizer.fit_transform(tur_test_articles)\n",
    "tur_test_all_terms = tur_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # disabling Named Entity Recognition for speed\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n",
    "    \n",
    "brief_cleaning = (re.sub(r'https?://\\S+|www\\.\\S+|\\[.*?\\]|<.*?>+|\\w*\\d\\w*|[{}]'.format(re.escape(string.punctuation)), ' ', str(row)).lower() for row in eng_train_df['Text'])\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_process=-1)]\n",
    "\n",
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import zeyrek\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('turkish')\n",
    "\n",
    "analyzer = zeyrek.MorphAnalyzer()\n",
    "\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "# Special thanks to https://www.kaggle.com/tanulsingh077 for this function\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "def stemm_text(text):\n",
    "    text = ' '.join(analyzer.lemmatize(word) for word in text.split(' '))\n",
    "    return text\n",
    "\n",
    "def preprocess_data(text):\n",
    "    # Clean puntuation, urls, and so on\n",
    "    text = clean_text(text)\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    # Stemm all the words in the sentence\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_train_df['Text_Clean'] = tur_train_df['Text'].apply(preprocess_data)\n",
    "tur_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_clean_df = tur_train_df['Text_Clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "sent = [row.split() for row in df_clean['clean']]\n",
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word, list_names):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\n",
    "    \"\"\"\n",
    "    arrays = np.empty((0, 300), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=15).fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=10).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tsnescatterplot(w2v_model, 'zionism', [i[0] for i in w2v_model.wv.most_similar(negative=[\"zionism\"])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LaD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
